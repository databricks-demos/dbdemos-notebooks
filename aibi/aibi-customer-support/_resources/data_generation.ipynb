{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fff7d7f-284f-4373-b88d-a8070268dd54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from faker.providers import BaseProvider\n",
    "\n",
    "cost_variability = {\n",
    "    \"hourly_rate_range\": (35, 65),  # Agent experience levels\n",
    "    \"peak_hour_multiplier\": {\n",
    "        \"morning\": 1.25,  # 8-11am\n",
    "        \"afternoon\": 1.0,  # 11am-3pm\n",
    "        \"evening\": 1.15,  # 3-7pm\n",
    "        \"night\": 1.4  # 7pm-8am\n",
    "    },\n",
    "    \"complexity_factors\": {\n",
    "        \"Hardware\": (0.8, 2.0),  # (min, max) multiplier\n",
    "        \"Software\": (0.9, 1.5),\n",
    "        \"Service\": (0.7, 1.8),\n",
    "        \"Training\": (0.4, 1.3),\n",
    "        \"Other\": (0.2, 1.2),\n",
    "    },\n",
    "    \"overtime_penalty\": 1.5,  # 50% extra after 8h\n",
    "    \"weekend_surcharge\": 1.3  # 30% weekend premium\n",
    "}\n",
    "# Progressive AI Performance Degradation Configuration (for billing cases only)\n",
    "progressive_ai_degradation = {\n",
    "    \"april_2025\": {\n",
    "        \"resolution_time_multiplier\": 1.80,  # 33% increase\n",
    "        \"ai_acceptance_drop\": 0.15,           # 15% drop in acceptance\n",
    "        \"first_resolution_drop\": 0.20         # 20% drop in FTR\n",
    "    },\n",
    "    \"may_2025\": {\n",
    "        \"resolution_time_multiplier\": 3.5,  # 80% increase  \n",
    "        \"ai_acceptance_drop\": 0.25,           # 25% drop in acceptance\n",
    "        \"first_resolution_drop\": 0.35         # 35% drop in FTR\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add to your existing configuration\n",
    "product_impacts = {\n",
    "    \"Hardware\": {\n",
    "        \"sla_penalty_multiplier\": 1.5,  # 50% higher penalties\n",
    "        \"upsell_probability\": 0.15,\n",
    "        \"clv_base\": 7000,\n",
    "        \"clv_std\": 2000,\n",
    "        \"cost_multiplier\": 1.2  # 20% higher resolution costs\n",
    "    },\n",
    "    \"Software\": {\n",
    "        \"sla_penalty_multiplier\": 1.0,\n",
    "        \"upsell_probability\": 0.25,\n",
    "        \"clv_base\": 5000,\n",
    "        \"clv_std\": 1500,\n",
    "        \"cost_multiplier\": 1.0\n",
    "    },\n",
    "    \"Service\": {\n",
    "        \"sla_penalty_multiplier\": 0.8,  # 20% lower penalties\n",
    "        \"upsell_probability\": 0.35,\n",
    "        \"clv_base\": 10000,\n",
    "        \"clv_std\": 3000,\n",
    "        \"cost_multiplier\": 0.9\n",
    "    },\n",
    "    \"Training\": {\n",
    "        \"sla_penalty_multiplier\": 0.7,  # 20% lower penalties\n",
    "        \"upsell_probability\": 0.45,\n",
    "        \"clv_base\": 10000,\n",
    "        \"clv_std\": 3000,\n",
    "        \"cost_multiplier\": 0.9\n",
    "    },\n",
    "    \"Other\": {\n",
    "        \"sla_penalty_multiplier\": 1,  # 20% lower penalties\n",
    "        \"upsell_probability\": 0.5,\n",
    "        \"clv_base\": 10000,\n",
    "        \"clv_std\": 3000,\n",
    "        \"cost_multiplier\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "# List of European countries with their approximate populations (in millions)\n",
    "european_countries = [\n",
    "    (\"Albania\", 2.8),\n",
    "    (\"Andorra\", 0.077),\n",
    "    (\"Armenia\", 3.0),\n",
    "    (\"Austria\", 8.9),\n",
    "    (\"Azerbaijan\", 10.0),\n",
    "    (\"Belarus\", 9.4),\n",
    "    (\"Belgium\", 11.5),\n",
    "    (\"Bosnia and Herzegovina\", 3.3),\n",
    "    (\"Bulgaria\", 6.9),\n",
    "    (\"Croatia\", 4.0),\n",
    "    (\"Cyprus\", 1.2),\n",
    "    (\"Czech Republic\", 10.7),\n",
    "    (\"Denmark\", 5.8),\n",
    "    (\"Estonia\", 1.3),\n",
    "    (\"Finland\", 5.5),\n",
    "    (\"France\", 67.4),\n",
    "    (\"Australia\", 26.4),\n",
    "    (\"Canada\", 40.1),\n",
    "    (\"Japan\", 124.1),\n",
    "    (\"United States\",340),\n",
    "    (\"Georgia\", 3.7),\n",
    "    (\"Germany\", 83.2),\n",
    "    (\"Greece\", 10.4),\n",
    "    (\"Hungary\", 9.7),\n",
    "    (\"Iceland\", 0.36),\n",
    "    (\"Ireland\", 4.9),\n",
    "    (\"Italy\", 59.3),\n",
    "    (\"Kazakhstan\", 18.8),\n",
    "    (\"Kosovo\", 1.8),\n",
    "    (\"Latvia\", 1.9),\n",
    "    (\"Liechtenstein\", 0.038),\n",
    "    (\"Lithuania\", 2.8),\n",
    "    (\"Luxembourg\", 0.63),\n",
    "    (\"Malta\", 0.51),\n",
    "    (\"Moldova\", 2.6),\n",
    "    (\"Monaco\", 0.039),\n",
    "    (\"Montenegro\", 0.62),\n",
    "    (\"Netherlands\", 17.5),\n",
    "    (\"North Macedonia\", 2.1),\n",
    "    (\"Norway\", 5.4),\n",
    "    (\"Poland\", 37.9),\n",
    "    (\"Portugal\", 10.3),\n",
    "    (\"Romania\", 19.2),\n",
    "    #(\"Russia\", 144.1),\n",
    "    (\"San Marino\", 0.034),\n",
    "    (\"Serbia\", 6.7),\n",
    "    (\"Slovakia\", 5.4),\n",
    "    (\"Slovenia\", 2.1),\n",
    "    (\"Spain\", 47.3),\n",
    "    (\"Sweden\", 10.3),\n",
    "    (\"Switzerland\", 8.6),\n",
    "    (\"Turkey\", 84.3),\n",
    "    (\"Ukraine\", 41.4),\n",
    "    (\"United Kingdom\", 67.2),\n",
    "    (\"Vatican City\", 0.0008)\n",
    "]\n",
    "\n",
    "# Extract countries and populations\n",
    "countries, populations = zip(*european_countries)\n",
    "\n",
    "# Custom provider for generating European countries based on population distribution\n",
    "class EuropeanCountryProvider(BaseProvider):\n",
    "    def european_country(self):\n",
    "        return random.choices(countries, weights=populations, k=1)[0]\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Add the custom provider to Faker\n",
    "fake.add_provider(EuropeanCountryProvider)\n",
    " \n",
    "np.random.seed(42)\n",
    "\n",
    "# --- Configuration ---\n",
    "n_tickets = 24472\n",
    "ai_ratio = 0.7  # 70% AI-assisted tickets\n",
    "n_agents = 16   # 8 AI, 8 human\n",
    "premium_ratio = 0.3  # 30% Premium support tickets\n",
    "base_hourly_cost = 45  # Average agent hourly rate\n",
    "ai_cost_multiplier = 0.4  # AI is 60% cheaper to operate\n",
    "premium_upsell_rate = 0.25  # 25% upsell chance on premium tickets\n",
    "\n",
    "# NEW: AI Performance Degradation Configuration\n",
    "ai_performance_degradation = {\n",
    "    \"ai_acceptance_drop\": 0.15,  # 15% drop in AI suggestion acceptance\n",
    "    \"billing_csat_impact\": -0.8,  # 0.8 point drop for billing cases\n",
    "    \"time_resolution_increase\": 1.25,  # 25% increase in resolution time\n",
    "    \"first_resolution_drop\": 0.20  # 20% drop in first-time resolution rate\n",
    "}\n",
    "\n",
    "# --- Create Agents ---\n",
    "ai_agents = [fake.unique.name() + \" (AI)\" for _ in range(n_agents//2)]\n",
    "human_agents = [fake.unique.name() for _ in range(n_agents//2)]\n",
    "all_agents = ai_agents + human_agents\n",
    "\n",
    "# --- Generate Unified Dataset ---\n",
    "data = {\n",
    "    \"status\": [],\n",
    "    \"ticket_id\": [],\n",
    "    \"priority\": [],\n",
    "    \"source\": [],\n",
    "    \"topic\": [],\n",
    "    \"agent_group\": [],\n",
    "    \"agent_name\": [],\n",
    "    \"created_time\": [],\n",
    "    \"expected_sla_to_resolve\": [],\n",
    "    \"expected_sla_to_first_response\": [],\n",
    "    \"first_response_time\": [],\n",
    "    \"sla_for_first_response\": [],\n",
    "    \"resolution_time\": [],\n",
    "    \"sla_for_resolution\": [],\n",
    "    \"close_time\": [],\n",
    "    \"agent_interactions\": [],  # This will store interaction quality scores\n",
    "    \"interaction_notes\": [],   # New: Textual notes\n",
    "    \"survey_results\": [],\n",
    "    \"product_group\": [],\n",
    "    \"support_level\": [],\n",
    "    \"country\": [],\n",
    "    \"latitude\": [],\n",
    "    \"longitude\": [],\n",
    "    \"call_transcript\": [],\n",
    "    \"compliance_greeting\": [],\n",
    "    \"compliance_farewell\": [],\n",
    "    \"compliance_data_leak\": [],\n",
    "    \"call_sentiment_score\": [],\n",
    "    \"compliance_score\": [],\n",
    "    # NEW FIELDS\n",
    "    \"ai_suggestion_accepted\": [],  # Only for AI-assisted agents\n",
    "    \"first_time_resolution\": [],   # Boolean: resolved on first contact\n",
    "    \"csat_score\": [],             # Customer satisfaction score (1-10)\n",
    "    \"resolution_attempt_number\": [] # How many attempts to resolve (1 = first time)\n",
    "}\n",
    "\n",
    "# Quality note templates\n",
    "good_notes = [\n",
    "    \"Used AI suggestions to quickly identify root cause\",\n",
    "    \"Automated troubleshooting steps applied\",\n",
    "    \"Knowledge base article referenced for standard solution\",\n",
    "    \"Escalated to specialist with detailed context\",\n",
    "    \"Proactive follow-up scheduled\"\n",
    "]\n",
    "\n",
    "bad_notes = [\n",
    "    \"Customer required multiple explanations\",\n",
    "    \"Had to consult supervisor for guidance\",\n",
    "    \"Solution required manual research\",\n",
    "    \"Back-and-forth communication needed\",\n",
    "    \"Documentation was unclear\"\n",
    "]\n",
    "\n",
    "# Add to your existing configuration\n",
    "compliance_rules = {\n",
    "    \"greeting\": [\"Hello\", \"Good [morning/afternoon]\", \"Thank you for calling\"],\n",
    "    \"farewell\": [\"Have a nice day\", \"Thank you for your time\"],\n",
    "    \"data_protection_phrases\": [\"Let me verify your identity\", \"For security purposes\"],\n",
    "    \"forbidden_phrases\": [\"I'm angry\", \"That's stupid\", \"I don't care\"]\n",
    "}\n",
    "\n",
    "def generate_transcript(is_ai):\n",
    "    \"\"\"Generate synthetic call transcript with compliance markers\"\"\"\n",
    "    transcript = []\n",
    "    \n",
    "    # --- Greeting ---\n",
    "    if is_ai or np.random.rand() < 0.8:  # AI has 95% compliance\n",
    "        transcript.append(np.random.choice([\n",
    "            \"Hello! Thank you for calling [Insurance Co]. How can I help you today?\",\n",
    "            \"Good morning! You've reached [Insurance Co]. May I have your policy number?\"\n",
    "        ]))\n",
    "    else:\n",
    "        if np.random.rand() < 0.2:  # 20% non-compliant greeting\n",
    "            transcript.append(\"Yeah, what do you need?\")\n",
    "        else:\n",
    "            transcript.append(\"Hi, [Insurance Co], policy number?\")\n",
    "\n",
    "    # --- Main Conversation ---\n",
    "    transcript.append(f\"Customer: {fake.sentence()}\")\n",
    "    \n",
    "    # AI agents reference knowledge base\n",
    "    if is_ai:\n",
    "        transcript.append(\"System: Suggested response - 'Let me verify that in our system'\")\n",
    "        transcript.append(\"Agent: Let me verify that in our system...\")\n",
    "    else:\n",
    "        transcript.append(\"Agent: Umm, let me check...\")\n",
    "    \n",
    "    # --- Data Protection ---\n",
    "    if is_ai or np.random.rand() < 0.9:\n",
    "        transcript.append(\"Agent: Could you confirm the last 4 digits of your SSN for verification?\")\n",
    "    else:\n",
    "        if np.random.rand() < 0.1:  # 10% data leak risk\n",
    "            transcript.append(\"Agent: So your full SSN is 123-45-6789 right?\")\n",
    "    \n",
    "    # --- Sentiment Control ---\n",
    "    if not is_ai and np.random.rand() < 0.15:  # 15% frustration risk\n",
    "        transcript.append(\"Agent: *sigh* I already explained this...\")\n",
    "    \n",
    "    # --- Farewell ---\n",
    "    if is_ai or np.random.rand() < 0.85:\n",
    "        transcript.append(np.random.choice([\n",
    "            \"Thank you for calling [Insurance Co]. Have a wonderful day!\",\n",
    "            \"We appreciate your business. Goodbye!\"\n",
    "        ]))\n",
    "    else:\n",
    "        transcript.append(\"Bye\")\n",
    "    \n",
    "    return \"\\n\".join(transcript)\n",
    "\n",
    "def calculate_financials(row):\n",
    "    product = row['product_group']\n",
    "    params = product_impacts[product]\n",
    "    \n",
    "    # Operational Costs\n",
    "    base_cost = base_hourly_cost * (row['resolution_time']/60)\n",
    "    if row['agent_group'] == \"AI-Assisted\":\n",
    "        cost_multiplier = ai_cost_multiplier\n",
    "    else:\n",
    "        cost_multiplier = 1.0\n",
    "        \n",
    "    cost_per_ticket = base_cost * cost_multiplier * params['cost_multiplier']\n",
    "    \n",
    "    # SLA Penalties\n",
    "    if row['sla_for_resolution'] == \"Breached SLA\":\n",
    "        base_penalty = 150 if row['priority'] == \"High\" else 75\n",
    "        sla_penalty = base_penalty * params['sla_penalty_multiplier']\n",
    "    else:\n",
    "        sla_penalty = 0\n",
    "        \n",
    "    # Upsell Revenue\n",
    "    if row['support_level'] == \"Premium\" and np.random.rand() < params['upsell_probability']:\n",
    "        upsell_revenue = np.random.choice([99, 199, 299]) * (1.5 if product == \"Service\" else 1.0)\n",
    "    else:\n",
    "        upsell_revenue = 0\n",
    "        \n",
    "    # CLV Calculations\n",
    "    clv_base = np.random.normal(params['clv_base'], params['clv_std'])\n",
    "    clv_risk = clv_base * (0.02 if row['survey_results'] < 3 else 0) * (1.2 if product == \"Service\" else 1.0)\n",
    "    \n",
    "    # --- Dynamic Hourly Rate ---\n",
    "    agent_exp = hash(row['agent_name']) % 10  # 0-9 experience level\n",
    "    base_rate = np.linspace(*cost_variability[\"hourly_rate_range\"], 10)[agent_exp]\n",
    "    \n",
    "    # --- Time-of-Day Premium ---\n",
    "    hour = row['created_time'].hour\n",
    "    if 8 <= hour < 11:\n",
    "        time_mult = cost_variability[\"peak_hour_multiplier\"][\"morning\"]\n",
    "    elif 11 <= hour < 15:\n",
    "        time_mult = 1.0\n",
    "    elif 15 <= hour < 19:\n",
    "        time_mult = cost_variability[\"peak_hour_multiplier\"][\"evening\"]\n",
    "    else:\n",
    "        time_mult = cost_variability[\"peak_hour_multiplier\"][\"night\"]\n",
    "    \n",
    "    # --- Product Complexity ---\n",
    "    product = row['product_group']\n",
    "    complexity = np.random.uniform(*cost_variability[\"complexity_factors\"][product])\n",
    "    \n",
    "    # --- Weekend/Overtime Costs ---\n",
    "    is_weekend = row['created_time'].weekday() >= 5\n",
    "    overtime = 1.0 + max(0, row['resolution_time'] - 8)/24 * cost_variability[\"overtime_penalty\"]\n",
    "    \n",
    "    # --- AI Cost Logic ---\n",
    "    if row['agent_group'] == \"AI-Assisted\":\n",
    "        cost_mult = ai_cost_multiplier * (0.9 + np.random.normal(0, 0.1))  # 10% AI variance\n",
    "    else:\n",
    "        cost_mult = 1.0 * (0.8 + np.random.normal(0, 0.2))  # 20% human variance\n",
    "    \n",
    "    # --- Final Calculation ---\n",
    "    raw_cost = base_rate * (row['resolution_time']/60)\n",
    "    final_cost = raw_cost * time_mult * complexity * overtime * cost_mult\n",
    "    \n",
    "    if is_weekend:\n",
    "        final_cost *= cost_variability[\"weekend_surcharge\"]\n",
    "    \n",
    "    return pd.Series([\n",
    "        final_cost,\n",
    "        sla_penalty,\n",
    "        upsell_revenue,\n",
    "        clv_base,\n",
    "        clv_risk\n",
    "    ])\n",
    "\n",
    "# NEW: Function to determine if issue is resolved on first attempt\n",
    "def calculate_first_time_resolution(topic, agent_group, ai_accepted=None, created_time=None):\n",
    "    \"\"\"Calculate if ticket is resolved on first contact\"\"\"\n",
    "    base_ftr_rate = 0.75  # 75% baseline first-time resolution\n",
    "    \n",
    "    # Topic impact on FTR\n",
    "    topic_multipliers = {\n",
    "        \"Feature request\": 0.9,\n",
    "        \"Bug report\": 0.8,\n",
    "        \"Other\": 0.85,\n",
    "        \"Billing Cases\": 0.8,  # Normal performance before April 2025\n",
    "        \"Pricing and licensing\": 0.7\n",
    "    }\n",
    "    \n",
    "    ftr_prob = base_ftr_rate * topic_multipliers.get(topic, 0.8)\n",
    "    \n",
    "    # AI assistance impact\n",
    "    if agent_group == \"AI-Assisted\":\n",
    "        if ai_accepted:\n",
    "            ftr_prob *= 1.1  # 10% boost when AI accepted\n",
    "        else:\n",
    "            ftr_prob *= 0.8  # 20% penalty when AI rejected\n",
    "    \n",
    "    # Apply progressive performance degradation ONLY to billing cases\n",
    "    if topic == \"Billing Cases\" and created_time and agent_group == \"AI-Assisted\":\n",
    "        if created_time >= datetime(2025, 5, 1):  # May 2025\n",
    "            ftr_prob *= (1 - progressive_ai_degradation[\"may_2025\"][\"first_resolution_drop\"])\n",
    "        elif created_time >= datetime(2025, 4, 1):  # April 2025\n",
    "            ftr_prob *= (1 - progressive_ai_degradation[\"april_2025\"][\"first_resolution_drop\"])\n",
    "    \n",
    "    return np.random.rand() < ftr_prob\n",
    "\n",
    "# NEW: Function to calculate CSAT score\n",
    "def calculate_csat_score(topic, first_time_resolution, agent_group, survey_results, created_time=None):\n",
    "    \"\"\"Calculate Customer Satisfaction (CSAT) score 1-10\"\"\"\n",
    "    base_csat = 7.2  # Average CSAT score\n",
    "    \n",
    "    # Topic-specific impact (billing cases hit hard ONLY from April 2025)\n",
    "    if topic == \"Billing Cases\" and created_time and created_time >= datetime(2025, 4, 1):\n",
    "        base_csat += ai_performance_degradation[\"billing_csat_impact\"]\n",
    "    \n",
    "    # First-time resolution bonus\n",
    "    if first_time_resolution:\n",
    "        base_csat += 0.8\n",
    "    else:\n",
    "        base_csat -= 0.5\n",
    "    \n",
    "    # Agent type impact\n",
    "    if agent_group == \"AI-Assisted\":\n",
    "        base_csat += 0.3  # Slight AI advantage\n",
    "    \n",
    "    # Correlation with survey results\n",
    "    csat = base_csat + (survey_results - 3) * 0.4\n",
    "    \n",
    "    # Add some randomness and constrain to 1-10\n",
    "    csat += np.random.normal(0, 0.5)\n",
    "    return np.clip(csat, 1, 10)\n",
    "\n",
    "for i in range(n_tickets):\n",
    "    # --- Common Fields ---\n",
    "    ticket_id = f\"{1000+i:04d}\"\n",
    "    created_time = fake.date_time_between(start_date=datetime(2024,6,1), end_date=datetime(2025,5,30))\n",
    "    is_premium = np.random.rand() < premium_ratio\n",
    "    \n",
    "    # Topic selection (keeping original distribution)\n",
    "    topic = np.random.choice([\"Feature request\",\"Bug report\",\"Other\",\"Billing Cases\",\"Pricing and licensing\"])\n",
    "    \n",
    "    # Assign agent\n",
    "    if np.random.rand() < ai_ratio:\n",
    "        agent_name = np.random.choice(ai_agents)\n",
    "        agent_group = \"AI-Assisted\"\n",
    "    else:\n",
    "        agent_name = np.random.choice(human_agents)\n",
    "        agent_group = \"Human Only\"\n",
    "    \n",
    "    is_ai = agent_group == \"AI-Assisted\"\n",
    "    \n",
    "    # NEW: AI Suggestion Acceptance (only for AI-assisted agents)\n",
    "    if is_ai:\n",
    "        base_acceptance_rate = 0.85  # 85% baseline acceptance\n",
    "        \n",
    "        # Apply progressive degradation ONLY to billing cases\n",
    "        if topic == \"Billing Cases\":\n",
    "            if created_time >= datetime(2025, 5, 1):  # May 2025\n",
    "                degraded_acceptance_rate = base_acceptance_rate * (1 - progressive_ai_degradation[\"may_2025\"][\"ai_acceptance_drop\"])\n",
    "            elif created_time >= datetime(2025, 4, 1):  # April 2025\n",
    "                degraded_acceptance_rate = base_acceptance_rate * (1 - progressive_ai_degradation[\"april_2025\"][\"ai_acceptance_drop\"])\n",
    "            else:\n",
    "                degraded_acceptance_rate = base_acceptance_rate  # Normal performance before April 2025\n",
    "        else:\n",
    "            degraded_acceptance_rate = base_acceptance_rate  # Normal performance for non-billing cases\n",
    "        \n",
    "        ai_suggestion_accepted = np.random.rand() < degraded_acceptance_rate\n",
    "    else:\n",
    "        ai_suggestion_accepted = None  # N/A for human-only agents\n",
    "        \n",
    "    # NEW: Calculate first-time resolution\n",
    "    first_time_resolution = calculate_first_time_resolution(topic, agent_group, ai_suggestion_accepted, created_time)\n",
    "    resolution_attempts = 1 if first_time_resolution else np.random.choice([2, 3, 4], p=[0.6, 0.3, 0.1])\n",
    "    \n",
    "    # --- Performance Logic (UPDATED with time-based degradation) ---\n",
    "    # Resolution times - apply increase ONLY from April 2025\n",
    "    if is_ai:\n",
    "        base_res_time = max(2, np.random.normal(3.5, 0.5))\n",
    "        \n",
    "        # Apply progressive performance degradation ONLY to billing cases\n",
    "        if topic == \"Billing Cases\":\n",
    "            if created_time >= datetime(2025, 5, 1):  # May 2025\n",
    "                base_res_time *= progressive_ai_degradation[\"may_2025\"][\"resolution_time_multiplier\"]\n",
    "            elif created_time >= datetime(2025, 4, 1):  # April 2025\n",
    "                base_res_time *= progressive_ai_degradation[\"april_2025\"][\"resolution_time_multiplier\"]\n",
    "            # Before April 2025: no degradation\n",
    "        \n",
    "        # Factor in AI acceptance\n",
    "        if ai_suggestion_accepted:\n",
    "            base_res_time *= 0.9  # 10% faster when AI accepted\n",
    "        else:\n",
    "            base_res_time *= 1.3  # 30% slower when AI rejected\n",
    "    else:\n",
    "        base_res_time = max(2, np.random.normal(6, 1))\n",
    "    \n",
    "    # Multiple attempts increase resolution time\n",
    "    if not first_time_resolution:\n",
    "        base_res_time *= resolution_attempts * 0.8\n",
    "    \n",
    "    if is_premium:\n",
    "        res_time = base_res_time * 0.6\n",
    "        support_level = \"Premium\"\n",
    "    else:\n",
    "        res_time = base_res_time\n",
    "        support_level = \"Basic\"\n",
    "    \n",
    "    # First response time\n",
    "    frt = max(0.1, np.random.normal(0.5, 0.2)) if is_ai else max(0.5, np.random.normal(2, 0.5))\n",
    "    \n",
    "    # --- Interaction Quality ---\n",
    "    if is_ai and is_premium:\n",
    "        # Best notes (AI + Premium)\n",
    "        note_quality = np.random.choice([4, 5])  # 4-5/5\n",
    "        notes = np.random.choice(good_notes, size=2, replace=False)\n",
    "    elif is_ai:\n",
    "        # Good notes (AI only) - but factor in acceptance\n",
    "        if ai_suggestion_accepted:\n",
    "            note_quality = np.random.choice([3, 4])  # 3-4/5\n",
    "            notes = [np.random.choice(good_notes)]\n",
    "        else:\n",
    "            note_quality = np.random.choice([2, 3])  # 2-3/5 when AI rejected\n",
    "            notes = [np.random.choice(bad_notes)]\n",
    "    elif is_premium:\n",
    "        # Medium notes (Premium only)\n",
    "        note_quality = np.random.choice([2, 3])  # 2-3/5\n",
    "        notes = [np.random.choice(good_notes + bad_notes)]\n",
    "    else:\n",
    "        # Poor notes (Human + Basic)\n",
    "        note_quality = np.random.choice([1, 2])  # 1-2/5\n",
    "        notes = np.random.choice(bad_notes, size=2, replace=False)\n",
    "    \n",
    "    # Format notes\n",
    "    formatted_notes = \" | \".join(notes)\n",
    "    \n",
    "    # Other metrics\n",
    "    survey = np.random.randint(3, 6) if is_ai else np.random.randint(1, 5)\n",
    "    \n",
    "    # NEW: Calculate CSAT score\n",
    "    csat_score = calculate_csat_score(topic, first_time_resolution, agent_group, survey, created_time)\n",
    "    \n",
    "    # --- Populate Data ---\n",
    "    data[\"ticket_id\"].append(ticket_id)\n",
    "    data[\"status\"].append(np.random.choice([\"Resolved\",\"Closed\",\"In progress\",\"Open\"], p=[0.3, 0.2, 0.4,0.1]))\n",
    "    data[\"priority\"].append(np.random.choice([\"Low\", \"Medium\", \"High\"], p=[0.5, 0.3, 0.2]))\n",
    "    data[\"source\"].append(np.random.choice([\"Phone\", \"Email\", \"Chat\"],p=[0.15, 0.55, 0.3]))\n",
    "    data[\"topic\"].append(topic)\n",
    "    data[\"agent_group\"].append(agent_group)\n",
    "    data[\"agent_name\"].append(agent_name)\n",
    "    data[\"created_time\"].append(created_time)\n",
    "    data[\"expected_sla_to_resolve\"].append(np.random.randint(4, 24))\n",
    "    data[\"expected_sla_to_first_response\"].append(np.random.randint(1, 4))\n",
    "    data[\"first_response_time\"].append(frt)\n",
    "    data[\"sla_for_first_response\"].append(\"Within SLA\" if frt <= data[\"expected_sla_to_first_response\"][-1] else \"Breached SLA\")\n",
    "    data[\"resolution_time\"].append(res_time)\n",
    "    data[\"sla_for_resolution\"].append(\"Within SLA\" if res_time <= data[\"expected_sla_to_resolve\"][-1] else \"Breached SLA\")\n",
    "    data[\"close_time\"].append(created_time + timedelta(hours=res_time))\n",
    "    data[\"agent_interactions\"].append(note_quality)  # Quality score 1-5\n",
    "    data[\"interaction_notes\"].append(formatted_notes)  # Text notes\n",
    "    data[\"survey_results\"].append(survey)\n",
    "    data[\"product_group\"].append(np.random.choice([\"Hardware\", \"Software\", \"Training\",\"Service\",\"Other\"],p=[0.1, 0.15, 0.45, 0.25, 0.05]))\n",
    "    data[\"support_level\"].append(support_level)\n",
    "    data[\"country\"].append(fake.european_country())\n",
    "    data[\"latitude\"].append(fake.latitude())\n",
    "    data[\"longitude\"].append(fake.longitude())\n",
    "    \n",
    "    # NEW FIELDS\n",
    "    data[\"ai_suggestion_accepted\"].append(ai_suggestion_accepted)\n",
    "    data[\"first_time_resolution\"].append(first_time_resolution)\n",
    "    data[\"csat_score\"].append(round(csat_score, 1))\n",
    "    data[\"resolution_attempt_number\"].append(resolution_attempts)\n",
    "\n",
    "    transcript = generate_transcript(is_ai)\n",
    "    \n",
    "    # AI Analysis Results\n",
    "    compliance_greeting = any(phrase in transcript for phrase in compliance_rules[\"greeting\"])\n",
    "    compliance_farewell = any(phrase in transcript for phrase in compliance_rules[\"farewell\"])\n",
    "    data_leak_risk = any(ssn in transcript for ssn in [\"SSN\", \"social security\"]) and not any(\n",
    "        phrase in transcript for phrase in compliance_rules[\"data_protection_phrases\"])\n",
    "    \n",
    "    sentiment_score = np.clip(np.random.normal(0.8 if is_ai else 0.4, 0.2), 0, 1)  # 0-1 scale\n",
    "    \n",
    "    # Add to data dictionary\n",
    "    data[\"call_transcript\"].append(transcript)\n",
    "    data[\"compliance_greeting\"].append(compliance_greeting)\n",
    "    data[\"compliance_farewell\"].append(compliance_farewell)\n",
    "    data[\"compliance_data_leak\"].append(data_leak_risk)\n",
    "    data[\"call_sentiment_score\"].append(sentiment_score)\n",
    "    data[\"compliance_score\"].append(\n",
    "        0.3*compliance_greeting + \n",
    "        0.2*compliance_farewell + \n",
    "        0.4*(not data_leak_risk) + \n",
    "        0.1*sentiment_score\n",
    "    )\n",
    "\n",
    "customer_review = pd.DataFrame(data)\n",
    "\n",
    "customer_review[[\n",
    "    'operational_cost',\n",
    "    'sla_penalty_cost',\n",
    "    'upsell_revenue',\n",
    "    'customer_clv',\n",
    "    'clv_risk'\n",
    "]] = customer_review.apply(calculate_financials, axis=1)\n",
    "\n",
    "# --- Add Churn Risk ---\n",
    "customer_review['churn_risk'] = np.where(\n",
    "    (customer_review['survey_results'] < 3) & \n",
    "    (customer_review['sla_for_resolution'] == \"Breached SLA\"),\n",
    "    np.random.uniform(0.15, 0.4),  # 15-40% churn risk\n",
    "    np.random.uniform(0.01, 0.1)    # 1-10% baseline\n",
    ")\n",
    "\n",
    "# --- Update SLA Table ---\n",
    "sla = customer_review[[\n",
    "    \"ticket_id\", \"expected_sla_to_resolve\", \"expected_sla_to_first_response\",\n",
    "    \"first_response_time\", \"sla_for_first_response\", \"resolution_time\",\n",
    "    \"sla_for_resolution\", \"survey_results\", \"sla_penalty_cost\",\"clv_risk\", \n",
    "    \"interaction_notes\", \"first_time_resolution\", \"csat_score\", \"resolution_attempt_number\"\n",
    "]].copy()\n",
    "\n",
    "# --- Split into Tables ---\n",
    "tickets = customer_review[[\n",
    "    \"ticket_id\", \"status\", \"priority\", \"source\", \"topic\",\n",
    "    \"created_time\", \"close_time\", \"product_group\",\n",
    "    \"support_level\", \"country\", \"latitude\", \"longitude\",\"operational_cost\",\"call_transcript\",\"compliance_greeting\",\n",
    "    \"compliance_data_leak\",\n",
    "    \"call_sentiment_score\", \"compliance_score\", \"csat_score\", \"first_time_resolution\"\n",
    "]].copy()\n",
    "\n",
    "agents = customer_review[[\n",
    "    \"ticket_id\", \"agent_group\", \"agent_name\", \"agent_interactions\", \"ai_suggestion_accepted\"\n",
    "]].copy()\n",
    "\n",
    "tickets[\"created_time\"] = tickets[\"created_time\"].astype(str)\n",
    "tickets[\"close_time\"] = tickets[\"close_time\"].astype(str)\n",
    "\n",
    "# --- Save to Parquet ---\n",
    "customer_review.to_parquet(f\"{volume_folder}/datasets/customer_review_axr/customer_review_axr.parquet\", engine=\"pyarrow\")\n",
    "tickets.to_parquet(f\"{volume_folder}/datasets/tickets_axr/tickets_axr.parquet\", engine=\"pyarrow\")\n",
    "sla.to_parquet(f\"{volume_folder}/datasets/sla_axr/sla_axr.parquet\", engine=\"pyarrow\")\n",
    "agents.to_parquet(f\"{volume_folder}/datasets/agents_axr/agents_axr.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\nPerformance Metrics Summary:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nPerformance Metrics Summary:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Split metrics by time period for comparison\n",
    "pre_april_2025 = customer_review[customer_review['created_time'] < datetime(2025, 4, 1)]\n",
    "post_april_2025 = customer_review[customer_review['created_time'] >= datetime(2025, 4, 1)]\n",
    "\n",
    "print(f\"\\nAI Suggestion Acceptance Rate:\")\n",
    "if len(pre_april_2025[pre_april_2025['agent_group'] == 'AI-Assisted']) > 0:\n",
    "    pre_acceptance = pre_april_2025[pre_april_2025['agent_group'] == 'AI-Assisted']['ai_suggestion_accepted'].mean()\n",
    "    print(f\"Before April 2025: {pre_acceptance:.2%}\")\n",
    "\n",
    "if len(post_april_2025[post_april_2025['agent_group'] == 'AI-Assisted']) > 0:\n",
    "    post_acceptance = post_april_2025[post_april_2025['agent_group'] == 'AI-Assisted']['ai_suggestion_accepted'].mean()\n",
    "    print(f\"April 2025 onwards: {post_acceptance:.2%}\")\n",
    "\n",
    "print(f\"\\nFirst-Time Resolution Rate:\")\n",
    "print(\"Before April 2025:\")\n",
    "if len(pre_april_2025) > 0:\n",
    "    print(pre_april_2025.groupby('agent_group')['first_time_resolution'].mean())\n",
    "print(\"April 2025 onwards:\")\n",
    "if len(post_april_2025) > 0:\n",
    "    print(post_april_2025.groupby('agent_group')['first_time_resolution'].mean())\n",
    "\n",
    "print(f\"\\nAverage Resolution Time by Agent Group:\")\n",
    "print(\"Before April 2025:\")\n",
    "if len(pre_april_2025) > 0:\n",
    "    print(pre_april_2025.groupby('agent_group')['resolution_time'].mean())\n",
    "print(\"April 2025 onwards:\")\n",
    "if len(post_april_2025) > 0:\n",
    "    print(post_april_2025.groupby('agent_group')['resolution_time'].mean())\n",
    "\n",
    "print(f\"\\nCSAT Score for Billing Cases:\")\n",
    "pre_billing = pre_april_2025[pre_april_2025['topic'] == 'Billing Cases']\n",
    "post_billing = post_april_2025[post_april_2025['topic'] == 'Billing Cases']\n",
    "\n",
    "if len(pre_billing) > 0:\n",
    "    print(f\"Before April 2025: {pre_billing['csat_score'].mean():.1f}\")\n",
    "if len(post_billing) > 0:\n",
    "    print(f\"April 2025 onwards: {post_billing['csat_score'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\nOverall CSAT Score by Agent Group:\")\n",
    "print(\"Before April 2025:\")\n",
    "if len(pre_april_2025) > 0:\n",
    "    print(pre_april_2025.groupby('agent_group')['csat_score'].mean())\n",
    "print(\"April 2025 onwards:\")\n",
    "if len(post_april_2025) > 0:\n",
    "    print(post_april_2025.groupby('agent_group')['csat_score'].mean())\n",
    "\n",
    "print(\"\\nInteraction Quality by Agent Type and Support Level:\")\n",
    "print(agents.merge(tickets[[\"ticket_id\", \"support_level\"]], on=\"ticket_id\")\n",
    "    .groupby([\"agent_group\", \"support_level\"])\n",
    "    [\"agent_interactions\"].mean())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "data_generation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
