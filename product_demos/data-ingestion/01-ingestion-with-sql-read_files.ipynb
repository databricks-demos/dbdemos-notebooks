{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "2cd31034-3555-447c-89c8-5c36546f4a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# What is Databricks SQL's read_files function?\n",
    "\n",
    "The read_files function in Databricks SQL is a powerful table-valued function that allows you to directly query and ingest data from files stored in cloud object storage or Unity Catalog volumes. It lets you treat raw data files as if they were tables, enabling immediate SQL-based analysis without requiring prior table creation or full data loading.\n",
    "\n",
    "This function is ideal for:\n",
    "\n",
    "Ad-hoc data exploration: Quickly examine data directly from files.\n",
    "\n",
    "Initial data ingestion: Serve as the source for creating streaming tables, especially when combined with Auto Loader for incremental processing.\n",
    "\n",
    "How read_files simplifies data access\n",
    "Accessing diverse file types at scale can be challenging. read_files makes it easy, offering these benefits:\n",
    "\n",
    "Direct Access: Read data directly from files in formats like JSON, CSV, Parquet, AVRO, ORC, and more.\n",
    "\n",
    "Automatic Schema Inference: It intelligently infers the schema across your files, or you can provide one.\n",
    "\n",
    "Flexible File Discovery: Easily target individual files or entire directories, including recursive discovery, using glob patterns.\n",
    "\n",
    "Simplified Data Engineering: Streamlines the initial step of bringing data into your Lakehouse, serving as a flexible entry point for your data pipelines.\n",
    "\n",
    "For more details, refer to the Databricks [read_files documentation](https://docs.databricks.com/aws/en/sql/language-manual/functions/read_files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aa10bfc-49e2-4a0d-a285-49fc36a4f285",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data initialization - run the cell to prepare the demo data."
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48bf8b1a-437d-46c9-94a1-3e6cbb971843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(volume_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbb18a6a-d249-46a4-93de-44c5422b1d9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1. Basic Usage: Automatic Format Dectection\n",
    "\n",
    "One of the key advantages of `read_files` is automatic format detection. Let's try to read many file formats in our demo data folder, and use read_files to detect the file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcaa743e-3ec0-439e-ab97-98770e127262",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading JSON files (auto-detected)"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_json') LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "132cad85-9e9c-4170-bd37-a63a6a00d301",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading CSV files (auto-detected)"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_csv') LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0534117-649a-42be-b168-ee8ed967690c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading parquet files (auto-detected)"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_parquet') LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd7a1440-dd6a-48fb-b74e-81b9e43197ee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Partitioned Parquet auto-detection"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT year, month, COUNT(*) as records FROM read_files('{volume_folder}/user_parquet_partitioned') GROUP BY year, month\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "455aadc4-b9cc-4f69-b59a-efedc79ced14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "`read_files` also supports powerful glob patterns for selective file reading. You can select the specific format you want to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a8ad6fe-2ecb-4146-ad25-ed34dd7800d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Basic glob patterns"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT 'JSON Files' as source, * FROM read_files('{volume_folder}/*json*') LIMIT 3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "367e5ea2-43d9-4269-95a9-6c16e2e05f6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 2. Schema Inference\n",
    "\n",
    "Different formats have different schema inference capabilities and performance.\n",
    "\n",
    "We can also use schema hints to override the schema inferrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b95ff6a4-e4f7-489e-b113-3159164848bb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON schema (inferred from data)"
    }
   },
   "outputs": [],
   "source": [
    "json_schema = spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_json') LIMIT 0\").schema\n",
    "print(json_schema.treeString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9cbb36c-c9b9-4699-bf86-181389a3c2ed",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV schema (inferred with headers)"
    }
   },
   "outputs": [],
   "source": [
    "csv_schema = spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_csv') LIMIT 0\").schema  \n",
    "print(csv_schema.treeString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e0eae61-4627-4352-8087-6513ec2407ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parquet schema (stored in metadata)"
    }
   },
   "outputs": [],
   "source": [
    "parquet_schema = spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_parquet') LIMIT 0\").schema\n",
    "print(parquet_schema.treeString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7a1f9f0-7d6e-4b3e-96c6-32c727c7482a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Type Precision Comparison"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "SELECT\n",
    "  format,\n",
    "  MAX(id_type) AS id_type,\n",
    "  MAX(age_group_type) AS age_group_type,\n",
    "  MAX(date_type) AS date_type\n",
    "FROM (\n",
    "SELECT \n",
    "  'JSON' as format,\n",
    "  typeof(id) as id_type,\n",
    "  typeof(age_group) as age_group_type,\n",
    "  typeof(creation_date) as date_type\n",
    "FROM read_files('{volume_folder}/user_json')\n",
    "UNION ALL\n",
    "SELECT \n",
    "  'CSV' as format,\n",
    "  typeof(id) as id_type, \n",
    "  typeof(age_group) as age_group_type,\n",
    "  typeof(creation_date) as date_type\n",
    "FROM read_files('{volume_folder}/user_csv')\n",
    "UNION ALL\n",
    "SELECT \n",
    "  'Parquet' as format,\n",
    "  typeof(id) as id_type,\n",
    "  typeof(age_group) as age_group_type, \n",
    "  typeof(creation_date) as date_type\n",
    "FROM read_files('{volume_folder}/user_parquet')\n",
    ") type_comparision\n",
    "GROUP BY format\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f45c46-2f1c-4caf-a773-4cb435ecc3d0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Using schema hints to override JSON inference"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  id,\n",
    "  typeof(id) as id_type_after_hint,\n",
    "  age_group,\n",
    "  typeof(age_group) as age_group_type_after_hint\n",
    "FROM read_files(\n",
    "  '{volume_folder}/user_json',\n",
    "  schemaHints => 'id bigint, age_group string'\n",
    ") LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5d42a10-3e53-46ef-a500-9187b527cbc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 3. Format-Specific Features\n",
    "\n",
    "There are some particular options that are specific to each format with `read_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "491331a5-ce88-49b4-b82a-7ee9733001a7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV without headers"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_csv_no_headers', format => 'csv', header => 'false') LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6e5397e-ad97-44d9-a2b8-41b0346e6900",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV without headers (manual schema)"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "SELECT * FROM read_files(\n",
    "  '{volume_folder}/user_csv_no_headers',\n",
    "  format => 'csv',\n",
    "  schema => 'id bigint, creation_date string, firstname string, lastname string, email string, address string, gender double, age_group double'\n",
    ") LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69be1dcb-bffc-49ef-b968-495e055e4615",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV with pipe delimiter"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "SELECT * FROM read_files(\n",
    "  '{volume_folder}/user_csv_pipe_delimited',\n",
    "  format => 'csv',\n",
    "  sep => '|'  \n",
    ") LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a78b050e-5e7f-4dc4-964e-7803186fb628",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON with column type inference"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  firstname,\n",
    "  lastname,\n",
    "  id,\n",
    "  typeof(id) as id_inferred_type,\n",
    "  age_group,\n",
    "  typeof(age_group) as age_group_inferred_type\n",
    "FROM read_files(\n",
    "  '{volume_folder}/user_json',\n",
    "  inferColumnTypes => true\n",
    ") LIMIT 5  \n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6867ae43-86ee-4620-b73d-fc467cb5c213",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parquet optimization features"
    }
   },
   "outputs": [],
   "source": [
    "# Demonstrate column pruning (Parquet's key advantage)\n",
    "print(\"⚡ Parquet Column Pruning Demo:\")\n",
    "import time\n",
    "\n",
    "# Read all columns\n",
    "start_time = time.time()\n",
    "all_cols_count = spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_parquet')\").count()\n",
    "all_cols_time = time.time() - start_time\n",
    "\n",
    "# Read only specific columns  \n",
    "start_time = time.time()\n",
    "select_cols_count = spark.sql(f\"SELECT id, firstname FROM read_files('{volume_folder}/user_parquet')\").count()\n",
    "select_cols_time = time.time() - start_time\n",
    "\n",
    "print(f\"📊 All columns: {all_cols_count:,} records in {all_cols_time:.2f}s\")\n",
    "print(f\"📊 2 columns: {select_cols_count:,} records in {select_cols_time:.2f}s\") \n",
    "print(f\"⚡ Column pruning speedup: {all_cols_time/select_cols_time:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "9c19e7a2-2be5-4766-87c6-c436bc1a6b42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 4. Streaming Usage\n",
    "\n",
    "`read_files` can be used in streaming tables to ingest files into Delta Lake. `read_files` leverages Auto Loader when used in a streaming table query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "529b5fbc-d8fb-461f-ae10-55712db0410f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Basic streaming example"
    }
   },
   "outputs": [],
   "source": [
    "# Create a streaming view\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW streaming_json_users AS\n",
    "SELECT \n",
    "  *,\n",
    "  current_timestamp() as processing_time\n",
    "FROM STREAM read_files(\n",
    "  '{volume_folder}/user_json',\n",
    "  maxFilesPerTrigger => 5,\n",
    "  schemaLocation => '{volume_folder}/read_files_streaming_schema'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "display(spark.sql(\"SELECT COUNT(*) as total_records FROM streaming_json_users\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c3c95a4-a632-4211-88dd-4fbc2c7df1be",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Multi-format streaming (separate streams with schema locations)"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW streaming_csv_users AS  \n",
    "SELECT \n",
    "  *,\n",
    "  'CSV' as source_format,\n",
    "  current_timestamp() as processing_time\n",
    "FROM STREAM read_files(\n",
    "  '{volume_folder}/user_csv',\n",
    "  schemaLocation => '{volume_folder}/csv_streaming_schema'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW streaming_parquet_users AS\n",
    "SELECT \n",
    "  *,\n",
    "  'PARQUET' as source_format, \n",
    "  current_timestamp() as processing_time\n",
    "FROM STREAM read_files(\n",
    "  '{volume_folder}/user_parquet',\n",
    "  schemaLocation => '{volume_folder}/parquet_streaming_schema'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "display(spark.sql(\"\"\"\n",
    "SELECT 'CSV' as format, COUNT(*) as records FROM streaming_csv_users\n",
    "UNION ALL  \n",
    "SELECT 'PARQUET' as format, COUNT(*) as records FROM streaming_parquet_users\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "b7bb4216-41a5-44cf-880b-d6e52328543f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 5. read_files vs Auto Loader\n",
    "\n",
    "We have covered some basic features of `read_files`. However, there might be some questions about when to use `read_files` and when to use Auto Loader.\n",
    "\n",
    "We have some comparison and decision matrix that could help you decide when to leverage the power of `read_files` and Auto Loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b243d62d-4829-4678-9e82-5596d75b031d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "| Capability | read_files | Auto Loader | Winner |\n",
    "|-----------|------------|-------------|---------|\n",
    "| Ad-hoc queries | ✅ Perfect | ❌ Not designed | read_files |\n",
    "| Multi-format API | ✅ Unified API | ❌ Single format | read_files |\n",
    "| Streaming performance | ⚠️ Basic | ✅ Optimized | Auto Loader |\n",
    "| Schema evolution | ⚠️ Manual | ✅ Automatic | Auto Loader |\n",
    "| Incremental processing | ❌ Full scan | ✅ Incremental | Auto Loader |\n",
    "| Setup complexity | ✅ Zero setup | ⚠️ More config | read_files |\n",
    "| Cost efficiency | ⚠️ Pay per query | ✅ Incremental | Auto Loader |\n",
    "| File notifications | ❌ No | ✅ Cloud notifications | Auto Loader |\n",
    "| Batch processing | ✅ Excellent | ⚠️ Streaming focus | read_files |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b9c5584-c57a-4a64-9dbe-910c03fa6aae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "| Scenario | Recommended Tool | Why |\n",
    "|----------|------------------|-----|\n",
    "| Data exploration | read_files | Zero setup, immediate results |\n",
    "| Production streaming | Auto Loader | Optimized for continuous ingestion |\n",
    "| Multi-format analysis | read_files | Unified API across formats |\n",
    "| One-time migration | read_files | Simple, no infrastructure setup |\n",
    "| Real-time pipelines | Auto Loader | Incremental processing, notifications |\n",
    "| Cross-format reporting | read_files | Query multiple formats easily |\n",
    "| Cost-sensitive streaming | Auto Loader | Only processes new data |\n",
    "| Quick prototyping | read_files | Fastest time to value |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "f14cd89e-37f9-4d9d-afb7-b2cb765f7eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "We have seen what the capabilities of Databricks SQL's `read_files` are, and now you can apply it in your projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62498f40-a4e7-4094-9e4e-489edc7546b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.stop_all_streams()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01-ingestion-with-sql-read_files",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
