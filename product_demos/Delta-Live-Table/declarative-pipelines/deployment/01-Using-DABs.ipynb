{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7757ce1c-ab22-444f-b776-1a7482bebdc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Asset Bundles\n",
    "Databricks Asset Bundles are the recommended approach to CI/CD on Databricks. Use Databricks Asset Bundles to describe Databricks resources such as jobs and pipelines as source files, and bundle them together with other assets to provide an end-to-end definition of a deployable project. These bundles of files can be source controlled, and you can use external CI/CD automation such as Github Actions to trigger deployments.\n",
    "\n",
    "You can use Databricks Asset Bundles to define and programmatically manage your Databricks CI/CD implementation, which usually includes:\n",
    "\n",
    "* **Notebooks**: Databricks notebooks are often a key part of data engineering and data science workflows. You can use version control for notebooks, and also validate and test them as part of a CI/CD pipeline. You can run automated tests against notebooks to check whether they are functioning as expected.\n",
    "* **Libraries**: Manage the library dependencies required to run your deployed code. Use version control on libraries and include them in automated testing and validation.\n",
    "* **Workflows**: Lakeflow Jobs are comprised of jobs that allow you to schedule and run automated tasks using notebooks or Spark jobs.\n",
    "* **Data pipelines**: You can also include data pipelines in CI/CD automation, using Lakeflow Declarative Pipelines, the framework in Databricks for declaring data pipelines.\n",
    "* **Infrastructure**: Infrastructure configuration includes definitions and provisioning information for clusters, workspaces, and storage for target environments. Infrastructure changes can be validated and tested as part of a CI/CD pipeline, ensuring that they are consistent and error-free.\n",
    "\n",
    "## Example Development Workflow\n",
    "\n",
    "<img src=\"https://docs.databricks.com/aws/en/assets/images/bundles-cicd-53be5f4860e8ebcedc2702f870290cda.png\" style=\"display: block; margin-left: auto; margin-right: auto; max-width: 100%;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0736834d-d2d6-4041-8f0b-e34f973acd59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## The basics of `databricks.yml`\n",
    "Each bundle must contain exactly one configuration file named `databricks.yml`, typically at the root of the project folder. The most simple `databricks.yml` you can create defines the bundle `name`, and a default `target`. For example:  \n",
    "```yml\n",
    "bundle:\n",
    "  name: my_bundle\n",
    "\n",
    "targets:\n",
    "  dev:\n",
    "    default: true\n",
    "```\n",
    "\n",
    "Resources you want to deploy can either be included directly in this `databricks.yml` or defined in additional yml files by using the `include` configuration. Here's an example of a simple asset bundle that deploys a job.\n",
    "\n",
    "```yml\n",
    "bundle:\n",
    "  name: my_bundle\n",
    "\n",
    "# Use include to break up bundle into multiple files. \n",
    "# Paths within a bundle are always relative to the yml they're defined in.\n",
    "include:\n",
    "  - resources/*.yml\n",
    "\n",
    "# Targets that you can deploy into\n",
    "# These can be different workspaces or different configurations of the pipeline\n",
    "targets:\n",
    "  dev:\n",
    "    default: true\n",
    "    workspace:\n",
    "      host: https://company.cloud.databricks.com\n",
    "\n",
    "\n",
    "# Resources can be defined in multiple files\n",
    "resources:\n",
    "  jobs:\n",
    "    # The resource name used here is used to reference this job elsewhere in your DAB if needed\n",
    "    default_python_job:\n",
    "      # Use the REST API Documentation to understand what configuration options are available for each resource type\n",
    "      name: default_python_job\n",
    "      tasks:\n",
    "        - task_key: notebook_task\n",
    "          notebook_task:\n",
    "            notebook_path: ../src/notebook.ipynb\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "893f81f7-45e4-4483-b95f-abb4ea5ad231",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Using Databricks Asset Bundles with Lakeflow Declaritive Pipelines\n",
    "For each of the resource types you can deploy using DABs, you can refer to the Databricks REST API documentation to see what fields are available. Here's a (incomplete) list of configuration options for pipelines today.\n",
    "```yml\n",
    "resources:\n",
    "    pipelines:\n",
    "        <pipeline-resource-name>: # DAB resource name for reference within asset bundle (this does not impact the deployed pipeline)\n",
    "            name: <pipeline name> # Friendly identifier for this pipeline.\n",
    "            catalog: <catalog> # A catalog in Unity Catalog to publish data from this pipeline to. \n",
    "            schema: <schema> # The default schema (database) where tables are read from or published to.\n",
    "            root_path: <pipeline root> # Relative path for the root of this pipeline. This is used as the root directory when editing the pipeline in the Databricks user interface and it is added to sys.path when executing Python sources during pipeline execution.\n",
    "            development: <true/false> # Whether the pipeline is in Development mode. Defaults to false.\n",
    "            libraries:\n",
    "                - glob:\n",
    "                    include: <path to folder or file> # Files to include as part of the pipeline. Path can be a notebook, a sql or python file or a folder path that ends in /**\n",
    "                - glob:\n",
    "                    include: <path to folder or file> # Multiple paths can be included here\n",
    "\n",
    "```\n",
    "\n",
    "See all the available configurations for pipelines [here](https://docs.databricks.com/api/workspace/pipelines/create)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2553310-c177-44e2-b2d2-6a7ed912d1b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Deploying this demo using DABs\n",
    "\n",
    "1. Install the Databricks CLI and authenticate to your workspace. [See the Databricks documentation for instructions on how to set the CLI up.](https://docs.databricks.com/aws/en/dev-tools/cli/tutorial)\n",
    "\n",
    "2. Download the pipeline-bike folder from your workspace onto your local computer. Use the \"Download as Zip (Notebook Source + File)\" option, and unzip the folder once it's downloaded.\n",
    "\n",
    "3. Copy `databricks.yml` from the deployment folder into the pipeline-bike folder and update the catalog, schema and workspace host values to reflect your environment.\n",
    "    ```yml\n",
    "    ...\n",
    "\n",
    "    variables:\n",
    "    catalog:\n",
    "      description: Default catalog that pipeline will publish assets to\n",
    "      default: <replace with your catalog>\n",
    "    schema:\n",
    "      description: Default schema that pipeline will publish assets to when no schema is specified in code\n",
    "      default: <replace with your schema>\n",
    "    \n",
    "    ...\n",
    "\n",
    "    targets:\n",
    "    dev:\n",
    "      mode: development\n",
    "      default: true\n",
    "      workspace:\n",
    "        host: <replace with your workspace>\n",
    "    \n",
    "    ...\n",
    "\n",
    "    ```\n",
    "\n",
    "4. From a terminal, run `databricks bundle validate`. If there are any errors, make sure to review them and address them as necessary.\n",
    "    ```bash\n",
    "      $ databricks bundle validate\n",
    "  \n",
    "      Name: pipeline-bike\n",
    "      Target: dev\n",
    "      Workspace:\n",
    "        Host: https://company.cloud.databricks.com/\n",
    "        User: user@company.com\n",
    "        Path: /Workspace/Users/user@company.com/.bundle/pipeline-bike/dev\n",
    "    ```\n",
    "\n",
    "5. Run `databricks bundle deploy` to deploy the bundle.\n",
    "    ```bash\n",
    "      $ databricks bundle deploy\n",
    "\n",
    "      Uploading bundle files to /Workspace/Users/user@company.com/.bundle/pipeline-bike/dev/files...\n",
    "      Deploying resources...\n",
    "      Updating deployment state...\n",
    "      Deployment complete!\n",
    "    ```\n",
    "\n",
    "6. Run `databricks bundle run generate_bike_data` to kick off a job to populate the raw data and start the pipeline.\n",
    "   ```bash\n",
    "    databricks bundle run generate_bike_data\n",
    "    Run URL: https://company.cloud.databricks.com/...\n",
    "\n",
    "    2025-08-29 14:35:33 \"[dev user_name] init-pipeline-bike\" RUNNING\n",
    "    2025-08-29 14:40:05 \"[dev user_name] init-pipeline-bike\" TERMINATED SUCCESS\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-Using-DABs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
