{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "72b5db75-7e4f-4eae-babe-f75fd6f994a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Ingesting data from any sources with Databricks Data Intelligence Platform\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/manufacturing/lakehouse-iot-turbine/di_platform_0.png\" style=\"float: left; margin-right: 30px\" width=\"600px\" />\n",
    "\n",
    "</br>\n",
    "\n",
    "Your data lives everywhere. Your insights shouldn't.\n",
    "\n",
    "In today's complex data landscape, critical business intelligence is often scattered across countless systems – from enterprise applications and databases to streaming feeds, APIs, and beyond. This fragmentation creates silos, delays insights, and hinders innovation.\n",
    "\n",
    "The Databricks Data Intelligence Platform changes that.\n",
    "\n",
    "With the Databricks Data Intelligence Platform, you can effortlessly ingest data from virtually any source, unifying your entire data estate into a single, intelligent foundation. Whether it's batch, streaming, or change data capture (CDC), we provide robust, scalable, and easy-to-use tools to bring all your data into the Lakehouse.\n",
    "\n",
    "Key capabilities include:\n",
    "\n",
    "- **Universal Connectivity**: Native connectors for enterprise applications (Salesforce, ServiceNow, Workday), databases (SQL Server, Oracle, PostgreSQL), cloud object storage (S3, ADLS, GCS), messaging queues (Kafka, Kinesis, Pub/Sub), and custom APIs.\n",
    "\n",
    "- **Automated & Incremental Ingestion**: Leverage powerful features like Auto Loader for efficient, incremental processing of new files as they arrive, and Lakeflow Connect for managed, serverless pipelines.\n",
    "\n",
    "- **Real-time Ready**: Seamlessly ingest and process high-throughput streaming data for immediate insights, real-time dashboards, and instant decision-making.\n",
    "\n",
    "- **Unified Governance**: Every ingested dataset is automatically governed by Unity Catalog, providing unified visibility, access control, lineage, and discovery across your entire data and AI assets.\n",
    "\n",
    "- **Simplified Data Engineering**: Build and manage data ingestion pipelines with ease using declarative frameworks like Lakeflow Declarative Pipelines (formely known as DLT), reducing complexity and accelerating time to value.\n",
    "\n",
    "Break down data silos, accelerate your data and AI initiatives, and unlock the full potential of your data with the Databricks Data Intelligence Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "b529c118-ebd2-44ed-b470-ee7a86ec6a51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1/ Ingest from Business Applications with Lakeflow connect\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/product/data-ingestion/lakeflow-connect.png\" style=\"float: left; margin-right: 30px; margin-top: 30px; margin-bottom: 30px;\" width=\"600px\" />\n",
    "\n",
    "**Lakeflow Connect** is the powerful ingestion component of Databricks Lakeflow, designed to simplify and accelerate bringing data from virtually any source into your Databricks Lakehouse. It offers a wide range of built-in, managed connectors for enterprise applications (like Salesforce, Workday, ServiceNow), databases (SQL Server, Oracle, PostgreSQL), cloud storage, and streaming sources.\n",
    "\n",
    "**Lakeflow Connect** streamlines the initial step of your data journey by providing an intuitive UI and API for quick setup, supporting incremental ingestion for efficiency, and leveraging serverless compute for scalable and cost-effective operations. It's deeply integrated with Unity Catalog for unified governance, ensuring that all ingested data is immediately discoverable, secure, and ready for analytics and AI.\n",
    "\n",
    "Take a look at Lakeflow Connect at the [Data Ingestion section](/ingestion/add)\n",
    "\n",
    "Or, take a product tour with the following use case of Lakeflow Connect\n",
    "\n",
    "- [**Databricks Lakeflow Connect for Salesforce:**](https://app.getreprise.com/launch/BXZjz8X/)\n",
    "Salesforce Platform Connector for Databricks Lakeflow Connect, enabling seamless integration with Salesforce to power advanced analytics and AI on CRM data, including support for custom objects and formula fields.\n",
    "\n",
    "- [**Databricks Lakeflow Connect for Workday Reports:**](https://app.getreprise.com/launch/ryNY32X/)\n",
    "Lakeflow Connect provides a streamlined way to ingest data from enterprise systems like Workday, alongside other sources such as cloud storage, databases, and local files. With a few clicks, you can configure pipelines that are not only quick to set up but also simple to maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "8391b180-1524-4bb1-977f-79b92d88bdb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 2/ Ingest data with SQL `read_file`\n",
    "\n",
    "### Instantly Access Any File with Databricks SQL's `read_files`\n",
    "\n",
    "**Unlock your data's potential, no matter where it lives or what format it's in.**\n",
    "\n",
    "The `read_files` function in Databricks SQL empowers you to directly query and analyze raw data files—from CSVs and JSONs to Parquet and more—stored in your cloud object storage or Unity Catalog volumes. Skip the complex setup and jump straight into insights.\n",
    "\n",
    "**Simply point, query, and transform.** `read_files` intelligently infers schemas, handles diverse file types, and integrates seamlessly with streaming tables for real-time ingestion. It's the fast, flexible way to bring all your files into the Databricks Lakehouse, accelerating your journey from raw data to actionable intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73123825-38ec-4450-a7d8-106c443f72bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Open [01-ingestion-with-sql-read_files]($./01-ingestion-with-sql-read_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af5a366d-d224-4207-8bf3-ff093de00f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 3/ Ingest files with Databricks Autoloader\n",
    "\n",
    "### Simplify Streaming Ingestion with Databricks Auto Loader\n",
    "\n",
    "**Effortlessly ingest new data as it arrives, without manual intervention.**\n",
    "\n",
    "Databricks Auto Loader is a powerful feature that automates and simplifies the process of incrementally and efficiently loading new data files from cloud storage into your Databricks Lakehouse. It's designed for streaming ingestion, ensuring your data is always up-to-date for real-time analytics and AI.\n",
    "\n",
    "**Set it and forget it.** Auto Loader intelligently detects and processes new files as they land in your configured cloud storage locations. It handles schema evolution, supports a wide range of file formats, and guarantees exactly-once processing for data integrity. Integrated seamlessly with Delta Live Tables and streaming tables, Auto Loader is the backbone for building robust, scalable, and fully automated data ingestion pipelines on Databricks. Focus on insights, not manual file management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "856cb034-0378-4232-b708-162f7cf8dbb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Open [02-Auto-loader-schema-evolution-Ingestion]($./02-Auto-loader-schema-evolution-Ingestion)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00-Ingestion-data-introduction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
