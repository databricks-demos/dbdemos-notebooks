{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "8eeeb251-5f27-4f06-99db-734f1c3b2371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Create feature tables in UC with Spark Declarative Pipelines\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/feature_store/2025Q4_03_image1.png?raw=true\" style=\"float: right\" width=\"500px\" />\n",
    "\n",
    "Any streaming table or materialized view in Unity Catalog with a primary key can be a feature table in Unity Catalog, and you can use the Features UI and API with the table.\n",
    "\n",
    "Below is an example of a full Spark Declarative Pipeline using feature store\n",
    "\n",
    "In order to run the notebook, please go to the pipeline UI to create a pipeline.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f68e88a-c4b2-441a-baca-10f263d7b5a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql import functions as F, Window as W\n",
    "from pyspark import pipelines as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48aa3313-49ea-4b08-9a78-5c95e876a167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dp.materialized_view(\n",
    "    name=\"travel_purchase_sdp\",\n",
    "    schema=\"\"\"\n",
    "        destination_id BIGINT NOT NULL,\n",
    "        user_id BIGINT NOT NULL,\n",
    "        ts TIMESTAMP,\n",
    "        clicked BOOLEAN,\n",
    "        purchased BOOLEAN,\n",
    "        price DOUBLE\n",
    "    \"\"\"\n",
    ")\n",
    "def travel_purchase_sdp():\n",
    "    df = (\n",
    "        spark.table(\"travel_purchase\")\n",
    "        .selectExpr(\n",
    "            \"CAST(destination_id AS BIGINT) AS destination_id\",\n",
    "            \"CAST(user_id AS BIGINT) AS user_id\",\n",
    "            \"ts\",\n",
    "            \"clicked\",\n",
    "            \"purchased\",\n",
    "            \"price\"\n",
    "        )\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "997edcc9-d313-451c-b032-52cb53e03a31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dp.materialized_view(\n",
    "    name=\"user_demography_sdp\",\n",
    "    schema=\"\"\"\n",
    "        user_id BIGINT NOT NULL,\n",
    "        age BIGINT,\n",
    "        gender STRING,\n",
    "        income_bracket STRING,\n",
    "        loyalty_tier STRING,\n",
    "        first_login_date TIMESTAMP,\n",
    "        billing_state STRING,\n",
    "        billing_city STRING\n",
    "    \"\"\"\n",
    ")\n",
    "def user_demography_sdp():\n",
    "    df = (\n",
    "        spark.table(\"user_demography\")\n",
    "        .selectExpr(\n",
    "            \"CAST(user_id AS BIGINT) AS user_id\",\n",
    "            \"CAST(age AS BIGINT) AS age\",\n",
    "            \"gender\",\n",
    "            \"income_bracket\",\n",
    "            \"loyalty_tier\",\n",
    "            \"first_login_date\",\n",
    "            \"billing_state\",\n",
    "            \"billing_city\"\n",
    "        )\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2f1927f-6994-4ccb-a226-fb8f7b976d95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dp.materialized_view(\n",
    "    name=\"destination_features_advanced_sdp\",\n",
    "    schema=\"\"\"\n",
    "        destination_id BIGINT NOT NULL,\n",
    "        ts TIMESTAMP,\n",
    "        sum_clicks_7d DOUBLE,\n",
    "        sum_impressions_7d DOUBLE,\n",
    "        CONSTRAINT destination_features_advanced_pk_sdp PRIMARY KEY (destination_id)\n",
    "    \"\"\"\n",
    ")\n",
    "def destination_features_advanced():\n",
    "    df = spark.table(\"travel_purchase_sdp\").select(\"destination_id\", \"ts\", \"clicked\")\n",
    "\n",
    "    window_spec = W.partitionBy(\"destination_id\").orderBy(F.col(\"ts\").cast(\"long\")).rangeBetween(-7*86400, 0)\n",
    "\n",
    "    df = (\n",
    "        df.withColumn(\"clicked_i\", F.col(\"clicked\").cast(\"int\"))\n",
    "          .withColumn(\"sum_clicks_7d\", F.sum(\"clicked_i\").over(window_spec).cast(\"double\"))\n",
    "          .withColumn(\"sum_impressions_7d\", F.count(\"*\").over(window_spec).cast(\"double\"))\n",
    "          # Fix datatype mismatches explicitly\n",
    "          .withColumn(\"destination_id\", F.col(\"destination_id\").cast(\"bigint\"))\n",
    "          .withColumn(\"ts\", F.col(\"ts\").cast(\"timestamp\"))\n",
    "          .select(\"destination_id\", \"ts\", \"sum_clicks_7d\", \"sum_impressions_7d\")\n",
    "    )\n",
    "    # Optional: fill nulls to avoid nullable conflicts\n",
    "    df = df.na.fill({\"sum_clicks_7d\": 0.0, \"sum_impressions_7d\": 0.0})\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deb2ac36-3186-44a9-aba2-8954d43f9011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dp.materialized_view(\n",
    "    name=\"user_features_advanced_sdp\",\n",
    "    schema=\"\"\"\n",
    "        user_id BIGINT NOT NULL,\n",
    "        ts TIMESTAMP,\n",
    "        mean_price_7d DOUBLE,\n",
    "        last_6m_purchases DOUBLE,\n",
    "        tenure_days DOUBLE,\n",
    "        age BIGINT,\n",
    "        gender STRING,\n",
    "        income_bracket STRING,\n",
    "        loyalty_tier STRING,\n",
    "        billing_state STRING,\n",
    "        billing_city STRING,\n",
    "        CONSTRAINT user_features_advanced_pk_sdp PRIMARY KEY (user_id)\n",
    "    \"\"\"\n",
    ")\n",
    "def user_features_advanced():\n",
    "    travel_purchase = spark.table(\"travel_purchase_sdp\").select(\"user_id\", \"price\", \"purchased\", \"ts\")\n",
    "    user_demo = spark.table(\"user_demography_sdp\")\n",
    "\n",
    "    window_spec = W.partitionBy(\"user_id\").orderBy(F.col(\"ts\").cast(\"long\"))\n",
    "\n",
    "    user_feat = (\n",
    "        travel_purchase\n",
    "        .withColumn(\"ts_l\", F.col(\"ts\").cast(\"long\"))\n",
    "        .withColumn(\n",
    "            \"lookedup_price_7d_rolling_sum\",\n",
    "            F.sum(\"price\").over(window_spec.rangeBetween(-7*86400, 0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"lookups_7d_rolling_sum\",\n",
    "            F.count(\"*\").over(window_spec.rangeBetween(-7*86400, 0))\n",
    "        )\n",
    "        .withColumn(\"mean_price_7d\", (F.col(\"lookedup_price_7d_rolling_sum\") / F.col(\"lookups_7d_rolling_sum\")).cast(\"double\"))\n",
    "        .withColumn(\"tickets_purchased\", F.col(\"purchased\").cast(\"int\"))\n",
    "        .withColumn(\n",
    "            \"last_6m_purchases\",\n",
    "            F.sum(\"tickets_purchased\").over(window_spec.rangeBetween(-6*30*86400, 0)).cast(\"double\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    user_feat = (\n",
    "        user_feat.join(user_demo, on=\"user_id\", how=\"left\")\n",
    "                 .withColumn(\"tenure_days\", F.datediff(F.current_date(), F.col(\"first_login_date\")).cast(\"double\"))\n",
    "                 .withColumn(\"user_id\", F.col(\"user_id\").cast(\"bigint\"))\n",
    "                 .withColumn(\"ts\", F.col(\"ts\").cast(\"timestamp\"))\n",
    "                 .select(\n",
    "                     \"user_id\", \"ts\", \"mean_price_7d\", \"last_6m_purchases\", \"tenure_days\",\n",
    "                     \"age\", \"gender\", \"income_bracket\", \"loyalty_tier\", \"billing_state\", \"billing_city\"\n",
    "                 )\n",
    "    )\n",
    "\n",
    "    # Optional: handle nulls for numeric columns to keep schema strict\n",
    "    user_feat = user_feat.na.fill({\n",
    "        \"mean_price_7d\": 0.0,\n",
    "        \"last_6m_purchases\": 0.0,\n",
    "        \"tenure_days\": 0.0\n",
    "    })\n",
    "\n",
    "    return user_feat\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_0c235d96-4bc7-4fb5-b118-17fd1dad0124",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Feature_store_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
