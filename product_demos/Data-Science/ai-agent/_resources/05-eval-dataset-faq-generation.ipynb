{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3534271-e1d8-4d17-837b-bdd44a7a4bfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Eval dataset for FAQ and documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "023cd9f1-a1ec-4a88-9d55-a185bcef898f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "105ca9cc-741a-4375-aada-1123ef202e4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'use catalog {catalog}')\n",
    "spark.sql(f'use schema {dbName}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a44f6d-77ce-44d6-919f-ec5c3b91076b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Generate a synthetic evaluation dataset for FAQ and questions about documentation and manuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86576658-ec1d-4e82-95c0-7f924e0a15d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, lit, from_json, explode, struct\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "# Step 1: Load 10 guides\n",
    "guides_df = spark.table(\"knowledge_base\").limit(10)\n",
    "\n",
    "# Step 2: Create prompt for question generation\n",
    "guides_with_prompts = guides_df.withColumn(\n",
    "    \"question_prompt\",\n",
    "    concat_ws(\n",
    "        \" \",\n",
    "        lit(\"You are building a question-answering dataset to evaluate an AI assistant trained on product documentation.\"),\n",
    "        lit(\"Based on the following technical guide, generate 3 realistic user questions.\"),\n",
    "        lit(\"Focus on error codes, troubleshooting, and step-by-step usage.\"),\n",
    "        lit(\"Return only a JSON array of questions.\"),\n",
    "        lit(\"Guide title:\"), col(\"title\"),\n",
    "        lit(\"Product:\"), col(\"product_name\"),\n",
    "        lit(\"Guide:\"), col(\"full_guide\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Step 3: Call AI_QUERY directly (in Python, no view)\n",
    "questions_raw = guides_with_prompts.select(\n",
    "    \"id\", \"product_name\", \"title\",\n",
    "    expr('AI_QUERY(\"databricks-claude-3-7-sonnet\", question_prompt)').alias(\"questions_json\")\n",
    ")\n",
    "\n",
    "# Step 4: Parse JSON → array<string>\n",
    "questions_parsed = questions_raw.withColumn(\"questions\", from_json(\"questions_json\", ArrayType(StringType())))\n",
    "\n",
    "# Step 5: Explode questions\n",
    "questions = questions_parsed.select(\"id\", \"product_name\", \"title\", explode(\"questions\").alias(\"question\"))\n",
    "# Step 6: Join back with full_guide\n",
    "questions_with_guides = questions.join(guides_df.select(\"id\", \"full_guide\"), on=\"id\", how=\"inner\")\n",
    "\n",
    "# Step 7: Build prompt to get expected facts\n",
    "questions_with_facts_prompt = questions_with_guides.withColumn(\n",
    "    \"fact_prompt\",\n",
    "    concat_ws(\n",
    "        \" \",\n",
    "        lit(\"You are evaluating an AI system. Based on the following product guide:\"),\n",
    "        col(\"full_guide\"),\n",
    "        lit(\"Return a JSON array of distinct factual statements (expected_facts) that would appear in a correct, helpful answer to the question.\"),\n",
    "        lit(\"Each fact must be concise, complete, and non-redundant.\"),\n",
    "        lit(\"Avoid repeating the same point in different words.\"),\n",
    "        lit(\"Use full, natural language sentences.\"),\n",
    "        lit(\"Return only the JSON array.\"),\n",
    "        lit(\"Question:\"), col(\"question\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 8: Call AI_QUERY again for expected_facts\n",
    "facts_raw = questions_with_facts_prompt.select(\n",
    "    \"question\",\n",
    "    expr('AI_QUERY(\"databricks-claude-3-7-sonnet\", fact_prompt)').alias(\"expected_facts_json\")\n",
    ")\n",
    "\n",
    "\n",
    "# Step 9: Parse JSON array → array<string>\n",
    "facts_df = facts_raw.withColumn(\"expected_facts\", from_json(\"expected_facts_json\", ArrayType(StringType())))\n",
    "\n",
    "# Step 10: Build final eval format\n",
    "eval_guides_df = facts_df.withColumn(\"inputs\", struct(\"question\")) \\\n",
    "                         .withColumn(\"predictions\", lit(\"\")) \\\n",
    "                         .withColumn(\"expectations\", struct(\"expected_facts\")) \\\n",
    "                         .select(\"inputs\", \"predictions\", \"expectations\")\n",
    "\n",
    "eval_guides_df.write.format('json').mode(\"append\").save(f\"/Volumes/{catalog}/{dbName}/{volume_name}/eval_dataset\")\n",
    "# Preview the result\n",
    "display(spark.read.json(f\"/Volumes/{catalog}/{dbName}/{volume_name}/eval_dataset\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_c75093c8-0895-475e-8c1b-6acacfe3368b",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "05-eval-dataset-faq-generation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
